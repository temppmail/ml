import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch

# Load the dataset
data = pd.read_csv('50_Startups.csv')

# Display the first few rows of the dataset
print(data.head())

# Data pre-processing
# Encode categorical variable 'STATE'
label_encoder = LabelEncoder()
data['STATE'] = label_encoder.fit_transform(data['STATE'])

# Selecting the relevant column for clustering
X = data[['PROFIT']]

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Use dendrogram to find the optimal number of clusters
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Start-ups')
plt.ylabel('Euclidean distances')
plt.show()

# Fit Agglomerative Hierarchical Clustering
num_clusters = 3  # You can choose the number of clusters based on the dendrogram
hc = AgglomerativeClustering(n_clusters=num_clusters, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X_scaled)

# Add the cluster labels to the dataset
data['cluster'] = y_hc

# Display the resulting clusters
print(data[['PROFIT', 'cluster']])

